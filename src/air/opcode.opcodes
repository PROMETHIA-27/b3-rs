# Copyright (C) 2015-2017 Apple Inc. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS''
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
# BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
# THE POSSIBILITY OF SUCH DAMAGE.

# Syllabus:
#
# Examples of some roles, types, and widths:
# U:G:32 => use of the low 32 bits of a general-purpose register or value
# D:G:32 => def of the low 32 bits of a general-purpose register or value
# UD:G:32 => use and def of the low 32 bits of a general-purpose register or value
# U:G:64 => use of the low 64 bits of a general-purpose register or value
# ZD:G:32 => def of all bits of a general-purpose register, where all but the low 32 bits are guaranteed to be zeroed.
# UA:G:Ptr => UseAddr (see comment in Arg.h)
# U:F:32 => use of a float register or value
# U:F:64 => use of a double register or value
# D:F:32 => def of a float register or value
# UD:F:32 => use and def of a float register or value
# S:F:32 => scratch float register.
#
# Argument kinds:
# Tmp => temporary or register
# Imm => 32-bit immediate int
# BigImm => TrustedImm64 on 64-bit targets, TrustedImm32 on 32-bit targets
# Addr => address as temporary/register+offset
# Index => BaseIndex address
# Abs => AbsoluteAddress
#
# The parser views these things as keywords, and understands that they fall into two distinct classes
# of things. So, although this file uses a particular indentation style, none of the whitespace or
# even newlines are meaningful to the parser. For example, you could write:
#
# Foo42 U:G:32, UD:F:32 Imm, Tmp Addr, Tmp
#
# And the parser would know that this is the same as:
#
# Foo42 U:G:32, UD:F:32
#     Imm, Tmp
#     Addr, Tmp
#
# I.e. a two-form instruction that uses a GPR or an int immediate and uses+defs a float register.
#
# Any opcode or opcode form can be preceded with an architecture list, which restricts the opcode to the
# union of those architectures. For example, if this is the only overload of the opcode, then it makes the
# opcode only available on x86_64:
#
# x86_64: Fuzz UD:G:64, D:G:64
#     Tmp, Tmp
#     Tmp, Addr
#
# But this only restricts the two-operand form, the other form is allowed on all architectures:
#
# x86_64: Fuzz UD:G:64, D:G:64
#     Tmp, Tmp
#     Tmp, Addr
# Fuzz UD:G:Ptr, D:G:Ptr, U:F:Ptr
#     Tmp, Tmp, Tmp
#     Tmp, Addr, Tmp
#
# And you can also restrict individual forms:
#
# Thingy UD:G:32, D:G:32
#     Tmp, Tmp
#     arm64: Tmp, Addr
#
# Additionally, you can have an intersection between the architectures of the opcode overload and the
# form. In this example, the version that takes an address is only available on armv7 while the other
# versions are available on armv7 or x86_64:
#
# x86_64 armv7: Buzz U:G:32, UD:F:32
#     Tmp, Tmp
#     Imm, Tmp
#     armv7: Addr, Tmp
#
# Finally, you can specify architectures using helpful architecture groups. Here are all of the
# architecture keywords that we support:
#
# x86: means x86-32 or x86-64.
# x86_32: means just x86-32.
# x86_64: means just x86-64.
# arm: means armv7 or arm64.
# armv7: means just armv7.
# arm64: means just arm64.
# 32: means x86-32 or armv7.
# 64: means x86-64 or arm64.

# Note that the opcodes here have a leading capital (Add32) but must correspond to MacroAssembler
# API that has a leading lower snake-case (add32). Generator automatically converts camel-case to snake-case.

Nop

Breakpoint

Add32 U:G:32, U:G:32, ZD:G:32
    Imm, Tmp, Tmp as add32_rrr
    Tmp, Tmp, Tmp as add32_rrr 

Add32 U:G:32, UZD:G:32
    Tmp, Tmp as add32
    x86: Imm, Addr as add32
    x86: Imm, Index as add32
    Imm, Tmp as add32
    x86: Addr, Tmp as add32
    x86: Index, Tmp as add32
    x86: Tmp, Addr as add32
    x86: Tmp, Index as add32

arm64: AddZeroExtend64 U:G:64, U:G:32, D:G:64
    Tmp, Tmp*, Tmp

arm64: AddSignExtend64 U:G:64, U:G:32, D:G:64
    Tmp, Tmp*, Tmp

x86: Add8 U:G:8, UD:G:8
    Imm, Addr as add8
    Imm, Index as add8
    Tmp, Addr as add8
    Tmp, Index as add8

x86: Add16 U:G:16, UD:G:16
    Imm, Addr as add16
    Imm, Index as add16
    Tmp, Addr as add16
    Tmp, Index as add16

64: Add64 U:G:64, UD:G:64
    Tmp, Tmp as add64
    x86: Imm, Addr as add64
    x86: Imm, Index as add64
    Imm, Tmp as add64
    x86: Addr, Tmp as add64
    x86: Index, Tmp as add64
    x86: Tmp, Addr as add64
    x86: Tmp, Index as add64

64: Add64 U:G:64, U:G:64, D:G:64
    Imm, Tmp, Tmp as add64_rrr 
    Tmp, Tmp, Tmp as add64_rrr

# note that this pseudoinstruction (and others like it) exist right now because
# air doesn't track live flags: we do support 64-bit addition on e.g. armv7
# using `adc` (add-with-carry)--however we must gurantee that Air does not
# change anything to disrupt the carry flag between the two adc instructions;
# so, for now, they are fused as this Add64

32: Add64 U:G:32, U:G:32, U:G:32, U:G:32, D:G:32, D:G:32
    Tmp, Tmp, Tmp, Tmp, Tmp, Tmp

AddDouble U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp as add_double
    x86: Addr, Tmp, Tmp as add_double 
    x86: Tmp, Addr, Tmp as add_double
    x86: Index, Tmp, Tmp as add_double 

x86: AddDouble U:F:64, UD:F:64
    Tmp, Tmp as add_double_rr
    Addr, Tmp as add_double_rr

AddFloat U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp as add_float
    x86: Addr, Tmp, Tmp as add_float 
    x86: Tmp, Addr, Tmp as add_float 
    x86: Index, Tmp, Tmp as add_float 

x86: AddFloat U:F:32, UD:F:32
    Tmp, Tmp as add_Float_rr 
    Addr, Tmp as add_float_rr

Sub32 U:G:32, UZD:G:32
    Tmp, Tmp as sub32
    x86: Imm, Addr as sub32 
    x86: Imm, Index as sub32 
    Imm, Tmp as sub32 
    x86: Addr, Tmp as sub32 
    x86: Index, Tmp as sub32 
    x86: Tmp, Addr as sub32 
    x86: Tmp, Index as sub32

arm: Sub32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as sub32_rrr
    Tmp, Imm, Tmp as sub32_rrr

64: Sub64 U:G:64, UD:G:64
    Tmp, Tmp as sub64
    x86: Imm, Addr as sub64
    x86: Imm, Index as sub64
    Imm, Tmp as sub64
    x86: Addr, Tmp as sub64
    x86: Index, Tmp as sub64
    x86: Tmp, Addr as sub64
    x86: Tmp, Index as sub64

arm64: Sub64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as sub64_rrr 
    Tmp, Imm, Tmp as sub64_rrr

# note see note above, on Add64
32: Sub64 U:G:32, U:G:32, U:G:32, U:G:32, D:G:32, D:G:32
    Tmp, Tmp, Tmp, Tmp, Tmp, Tmp 

SubDouble U:F:64, U:F:64, D:F:64
    arm x86_64_avx: Tmp, Tmp, Tmp as sub_double
    x86: Tmp, Addr, Tmp as sub_double 
    x86: Tmp, Index, Tmp as sub_double

x86: SubDouble U:F:64, UD:F:64
    Tmp, Tmp as sub_double_rr
    Addr, Tmp as sub_double_rr

SubFloat U:F:32, U:F:32, D:F:32
    arm x86_64_avx: Tmp, Tmp, Tmp as sub_float
    x86: Tmp, Addr, Tmp as sub_float 
    x86: Tmp, Index, Tmp as sub_float 

x86: SubFloat U:F:32, UD:F:32
    Tmp, Tmp as sub_float_rr
    Addr, Tmp as sub_float_rr

Neg32 UZD:G:32
    Tmp as neg32 
    x86: Addr as neg32 
    x86: Index as neg32

64: Neg64 UD:G:64
    Tmp as neg64 
    x86: Addr as neg64 
    x86: Index as neg64

arm: NegateDouble U:F:64, D:F:64
    Tmp, Tmp as negate_float

arm: NegateFloat U:F:32, D:F:32
    Tmp, Tmp as negate_float

Mul32 U:G:32, UZD:G:32
    Tmp, Tmp as mul32
    x86: Addr, Tmp as mul32 

Mul32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as mul32_rrr 
    x86: Addr, Tmp, Tmp as mul32_rrr 
    x86: Tmp, Addr, Tmp as mul32_rrr 
    x86: Imm, Tmp, Tmp as mul32_rrr

32: UMull32 U:G:32, U:G:32, ZD:G:32, ZD:G:32
    Tmp, Tmp, Tmp, Tmp

64: Mul64 U:G:64, UD:G:64
    Tmp, Tmp as mul64

64: Mul64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as mul64_rrr

arm64: MultiplyAdd32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp, Tmp as multiply_add32_rrrr

arm64: MultiplyAdd64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp, Tmp as multiply_add64_rrrr

arm64: MultiplyAddSignExtend32 U:G:32, U:G:32, U:G:64, D:G:64
    Tmp, Tmp, Tmp, Tmp as multiply_add_sign_extend32_rrrr

arm64: MultiplyAddZeroExtend32 U:G:32, U:G:32, U:G:64, D:G:64
    Tmp, Tmp, Tmp, Tmp as multiply_add_zero_extend32_rrrr

arm64: MultiplySub32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp, Tmp as multiply_sub32_rrrr

arm64: MultiplySub64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp, Tmp as multiply_sub64_rrrr

arm64: MultiplySubSignExtend32 U:G:32, U:G:32, U:G:64, D:G:64
    Tmp, Tmp, Tmp, Tmp as multiply_sub_sign_extend32_rrrr

arm64: MultiplySubZeroExtend32 U:G:32, U:G:32, U:G:64, D:G:64
    Tmp, Tmp, Tmp, Tmp as multiply_sub_zero_extend32_rrrr

arm64: MultiplyNeg32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as multiply_neg32_rrr

arm64: MultiplyNeg64 U:G:64, U:G:64, ZD:G:64
    Tmp, Tmp, Tmp as multiply_neg64_rrr

arm64: MultiplyNegSignExtend32 U:G:32, U:G:32, D:G:64
    Tmp, Tmp, Tmp as multiply_neg_sign_extend32_rrr

arm64: MultiplyNegZeroExtend32 U:G:32, U:G:32, D:G:64
    Tmp, Tmp, Tmp as multiply_neg_zero_extend32_rrr

arm64: MultiplySignExtend32 U:G:32, U:G:32, D:G:64
    Tmp, Tmp, Tmp as multiply_sign_extend32_rrr

arm64: MultiplyZeroExtend32 U:G:32, U:G:32, D:G:64
    Tmp, Tmp, Tmp as multiply_zero_extend32_rrr

arm64: Div32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as div32_rrr

arm64: UDiv32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as udiv32_rrr

arm64: Div64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as div64_rrr

arm64: UDiv64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as udiv64_rrr

MulDouble U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp as mul_double
    x86: Addr, Tmp, Tmp as mul_double
    x86: Tmp, Addr, Tmp as mul_double
    x86: Index, Tmp, Tmp as mul_double

x86: MulDouble U:F:64, UD:F:64
    Tmp, Tmp as mul_double_rr 
    Addr, Tmp as mul_double_rr

MulFloat U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp as mul_float
    x86: Addr, Tmp, Tmp as mul_float
    x86: Tmp, Addr, Tmp as mul_float
    x86: Index, Tmp, Tmp as mul_float

x86: MulFloat U:F:32, UD:F:32
    Tmp, Tmp as mul_float_rr
    Addr, Tmp as mul_float_rr

arm x86_64_avx: DivDouble U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp as div_double_rrr

x86: DivDouble U:F:64, UD:F:64
    Tmp, Tmp as div_double 
    Addr, Tmp as div_double

arm x86_64_avx: DivFloat U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp as div_float_rrr

x86: DivFloat U:F:32, UD:F:32
    Tmp, Tmp as div_float
    Addr, Tmp as div_float

x86: X86ConvertToDoubleWord32 U:G:32, ZD:G:32
    Tmp*, Tmp* as x86_convert_to_double_word32_rr

x86_64: X86ConvertToQuadWord64 U:G:64, D:G:64
    Tmp*, Tmp* as x86_convert_to_quad_word64_rr

x86: X86Div32 UZD:G:32, UZD:G:32, U:G:32
    Tmp*, Tmp*, Tmp as x86div32_rrr

x86: X86UDiv32 UZD:G:32, UZD:G:32, U:G:32
    Tmp*, Tmp*, Tmp as x86udiv32_rrr

x86_64: X86Div64 UZD:G:64, UZD:G:64, U:G:64
    Tmp*, Tmp*, Tmp as x86div64_rrr

x86_64: X86UDiv64 UZD:G:64, UZD:G:64, U:G:64
    Tmp*, Tmp*, Tmp as x86udiv64_rrr

# If we add other things like Lea that are UA, we may need to lower
# them on arm64 similarly to how we do for Lea. In lowerStackArgs,
# we lower Lea to add on arm64.
Lea32 UA:G:32, D:G:32
    Addr, Tmp as lea32
    x86: Index, Tmp as x86_lea32

64: Lea64 UA:G:64, D:G:64
    Addr, Tmp as lea64
    x86: Index, Tmp as x86_lea64

And32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as and32_rrr
    arm64: BitImm, Tmp, Tmp as and32_rrr
    32:  Imm, Tmp, Tmp as and32_rr
    x86: Tmp, Addr, Tmp as and32_rrr
    x86: Addr, Tmp, Tmp as and32_rrr

And32 U:G:32, UZD:G:32
    Tmp, Tmp as and32 
    x86: Imm, Tmp as and32
    x86: Tmp, Addr as and32
    x86: Tmp, Index as and32
    x86: Addr, Tmp as and32
    x86: Index, Tmp as and32
    x86: Imm, Addr as and32
    x86: Imm, Index as and32

64: And64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as and64_rrr
    arm64: BitImm64, Tmp, Tmp as and64_rrr

x86_64: And64 U:G:64, UD:G:64
    Tmp, Tmp as and64 
    Imm, Tmp as and64
    Imm, Addr as and64
    Imm, Index as and64
    Tmp, Addr as and64
    Tmp, Index as and64
    Addr, Tmp as and64
    Index, Tmp as and64

AndDouble U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp as and_double

x86: AndDouble U:F:64, UD:F:64
    Tmp, Tmp as and_double_rr

AndFloat U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp as and_float

x86: AndFloat U:F:32, UD:F:32
    Tmp, Tmp as and_float_rr

OrDouble U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp as or_double_rrr

x86: OrDouble U:F:64, UD:F:64
    Tmp, Tmp as or_double

OrFloat U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp as or_float_rrr

x86: OrFloat U:F:32, UD:F:32
    Tmp, Tmp as or_float

x86: XorDouble U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp as xor_double

x86: XorDouble U:F:64, UD:F:64
    Tmp, Tmp as xor_double_rr

x86: XorFloat U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp as xor_float

x86: XorFloat U:F:32, UD:F:32
    Tmp, Tmp as xor_float_rr

arm: Lshift32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as lshift32_rrr
    Tmp, Imm, Tmp as lshift32_rrr

x86:Lshift32 U:G:32, UZD:G:32
    Tmp*, Tmp as lshift32
    Imm, Tmp as lshift32

arm64: Lshift64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as lshift64_rrr
    Tmp, Imm, Tmp as lshift64_rrr

x86_64: Lshift64 U:G:64, UD:G:64
    Tmp*, Tmp as lshift64
    Imm, Tmp as lshift64

arm: Rshift32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as rshift32_rrr
    Tmp, Imm, Tmp as rshift32_rrr

x86: Rshift32 U:G:32, UZD:G:32
    Tmp*, Tmp as rshift32
    Imm, Tmp as rshift32 

arm64: Rshift64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as rshift64_rrr
    Tmp, Imm, Tmp as rshift64_rrr

x86_64: Rshift64 U:G:64, UD:G:64
    Tmp*, Tmp as rshift64
    Imm, Tmp as rshift64

arm: Urshift32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as urshift32_rrr
    Tmp, Imm, Tmp as urshift32_rrr

x86: Urshift32 U:G:32, UZD:G:32
    Tmp*, Tmp as urshift32
    Imm, Tmp as urshift32

arm64: Urshift64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as urshift64_rrr
    Tmp, Imm, Tmp as urshift64_rrr

x86_64: Urshift64 U:G:64, UD:G:64
    Tmp*, Tmp as urshift64
    Imm, Tmp as urshift64

x86_64: RotateRight32 U:G:32, UZD:G:32
    Tmp*, Tmp as rotate_right32
    Imm, Tmp as rotate_right32

arm: RotateRight32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as rotate_right32_rrr
    Tmp, Imm, Tmp as rotate_right32_rrr

x86_64: RotateRight64 U:G:64, UD:G:64
    Tmp*, Tmp as rotate_right64
    Imm, Tmp as rotate_right64

arm64: RotateRight64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as rotate_right64_rrr
    Tmp, Imm, Tmp as rotate_right64_rrr

x86_64: RotateLeft32 U:G:32, UZD:G:32
    Tmp*, Tmp as rotate_left32 
    Imm, Tmp as rotate_left32

x86_64: RotateLeft64 U:G:64, UD:G:64
    Tmp*, Tmp as rotate_left64
    Imm, Tmp as rotate_left64

Or32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as or32_rrr
    arm64: BitImm, Tmp, Tmp as or32_rrr 
    x86: Tmp, Addr, Tmp as or32_rrr 
    x86: Addr, Tmp, Tmp as or32_rrr

Or32 U:G:32, UZD:G:32
    Tmp, Tmp as or32 
    x86: Imm, Tmp as or32 
    x86: Tmp, Addr as or32 
    x86: Tmp, Index as or32 
    x86: Addr, Tmp as or32 
    x86: Index, Tmp as or32 
    x86: Imm, Addr as or32 
    x86: Imm, Index as or32

64: Or64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as or64_rrr
    arm64: BitImm64, Tmp, Tmp as or64_rrr

64: Or64 U:G:64, UD:G:64
    Tmp, Tmp as or64 
    x86: Imm, Tmp as or64 
    x86: Imm, Addr as or64 
    x86: Imm, Index as or64 
    x86: Tmp, Addr as or64 
    x86: Tmp, Index as or64 
    x86: Addr, Tmp as or64 
    x86: Index, Tmp as or64

Xor32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp as xor32_rrr
    arm64: BitImm, Tmp, Tmp as xor32_rrr
    x86: Tmp, Addr, Tmp as xor32_rrr
    x86: Addr, Tmp, Tmp as xor32_rrr

Xor32 U:G:32, UZD:G:32
    Tmp, Tmp as xor32 
    x86: Imm, Tmp as xor32 
    x86: Tmp, Addr as xor32 
    x86: Tmp, Index as xor32 
    x86: Addr, Tmp as xor32 
    x86: Index, Tmp as xor32 
    x86: Imm, Addr as xor32 
    x86: Imm, Index as xor32

64: Xor64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp as xor64_rrr
    arm64: BitImm64, Tmp, Tmp as xor64_rrr

64: Xor64 U:G:64, UD:G:64
    Tmp, Tmp as xor64 
    x86: Tmp, Addr as xor64 
    x86: Tmp, Index as xor64 
    x86: Addr, Tmp as xor64 
    x86: Index, Tmp as xor64 
    x86: Imm, Addr as xor64 
    x86: Imm, Index as xor64 
    x86: Imm, Tmp as xor64

arm: Not32 U:G:32, ZD:G:32
    Tmp, Tmp as not32

x86: Not32 UZD:G:32
    Tmp as not32 
    Addr as not32 
    Index as not32

arm64: Not64 U:G:64, D:G:64
    Tmp, Tmp as not64

x86_64: Not64 UD:G:64
    Tmp as not64 
    Addr as not64 
    Index as not64

arm: AbsDouble U:F:64, D:F:64
    Tmp, Tmp as abs_double

arm: AbsFloat U:F:32, D:F:32
    Tmp, Tmp as abs_float

CeilDouble U:F:64, D:F:64
    Tmp, Tmp as ceil_double
    x86: Addr, Tmp as ceil_double

CeilFloat U:F:32, D:F:32
    Tmp, Tmp as ceil_float
    x86: Addr, Tmp as ceil_float

FloorDouble U:F:64, D:F:64
    Tmp, Tmp as floor_double
    x86: Addr, Tmp as floor_double

FloorFloat U:F:32, D:F:32
    Tmp, Tmp as floor_float
    x86: Addr, Tmp as floor_float

SqrtDouble U:F:64, D:F:64
    Tmp, Tmp as sqrt_double 
    x86: Addr, Tmp as sqrt_double

SqrtFloat U:F:32, D:F:32
    Tmp, Tmp as sqrt_float
    x86: Addr, Tmp as sqrt_float

ConvertInt32ToDouble U:G:32, D:F:64
    Tmp, Tmp as convert_int32_to_double
    x86: Addr, Tmp as convert_int32_to_double

64: ConvertInt64ToDouble U:G:64, D:F:64
    Tmp, Tmp as convert_int64_to_double
    x86_64: Addr, Tmp as convert_int64_to_double

ConvertInt32ToFloat U:G:32, D:F:32
    Tmp, Tmp as convert_int32_to_float
    x86: Addr, Tmp as convert_int32_to_float

64: ConvertInt64ToFloat U:G:64, D:F:32
    Tmp, Tmp as convert_int64_to_float
    x86_64: Addr, Tmp as convert_int64_to_float

CountLeadingZeros32 U:G:32, ZD:G:32
    Tmp, Tmp as count_leading_zeros32
    x86: Addr, Tmp as count_leading_zeros32

64: CountLeadingZeros64 U:G:64, D:G:64
    Tmp, Tmp as count_leading_zeros64
    x86: Addr, Tmp as count_leading_zeros64

ConvertDoubleToFloat U:F:64, D:F:32
    Tmp, Tmp as convert_double_to_float
    x86: Addr, Tmp as convert_double_to_float

ConvertFloatToDouble U:F:32, D:F:64
    Tmp, Tmp as convert_float_to_double 
    x86: Addr, Tmp as convert_float_to_double

# Note that Move operates over the full register size, which is either 32-bit or 64-bit depending on
# the platform. I'm not entirely sure that this is a good thing; it might be better to just have a
# Move64 instruction. OTOH, our MacroAssemblers already have this notion of "move()" that basically
# means movePtr.
Move U:G:Ptr, D:G:Ptr
    Tmp, Tmp as mov 
    Imm, Tmp as sign_extend32_to_64
    BigImm, Tmp as mov
    Addr, Tmp as load64 # This means that "Move Addr, Tmp" is code-generated as "load" not "move".
    Index, Tmp as load64
    Tmp, Addr as store64
    Tmp, Index as store64
    x86: Imm, Addr as store64
    arm64: ZeroReg, Tmp as mov
    arm64: ZeroReg, Addr as store64
    arm64: ZeroReg, Index as store64

# This is for moving between spill slots.
Move U:G:Ptr, D:G:Ptr, S:G:Ptr
    Addr, Addr, Tmp as move_rrr

x86: Swap32 UD:G:32, UD:G:32
    Tmp, Tmp as swap32 
    Tmp, Addr as swap32

x86_64: Swap64 UD:G:64, UD:G:64
    Tmp, Tmp as swap64 
    Tmp, Addr as swap64

arm64: MoveWithIncrement64 UD:G:64, D:G:64
    PreIndex, Tmp as load64
    PostIndex, Tmp as load64
    Tmp, PreIndex as store64
    Tmp, PostIndex as store64

Move32 U:G:32, ZD:G:32
    Tmp, Tmp as zero_extend32_to_word
    Addr, Tmp as load32
    Index, Tmp as load32
    Tmp, Addr as store32
    Tmp, Index as store32
    x86: Imm, Tmp as zero_extend32_to_word
    x86 armv7: Imm, Addr as store32
    x86: Imm, Index as store32

arm64: MoveWithIncrement32 UD:G:32, ZD:G:32
    PreIndex, Tmp as load32
    PostIndex, Tmp as load32
    Tmp, PreIndex as store32
    Tmp, PostIndex as store32

# This is for moving between spill slots.
Move32 U:G:32, ZD:G:32, S:G:32
    Addr, Addr, Tmp as move32_rrr

arm64: Store32 U:G:32, ZD:G:32
    ZeroReg, Addr
    ZeroReg, Index

arm64: Store64 U:G:64, D:G:64
    ZeroReg, Addr
    ZeroReg, Index

64: SignExtend8To64 U:G:8, D:G:64
    Tmp, Tmp as sign_extend8_to_64

64: SignExtend16To64 U:G:16, D:G:64
    Tmp, Tmp as sign_extend16_to_64

64: SignExtend32To64 U:G:32, D:G:64
    Tmp, Tmp as sign_extend32_to_64

ZeroExtend8To32 U:G:8, ZD:G:32
    Tmp, Tmp as zero_extend8_to_32
    x86: Addr, Tmp as load8
    x86: Index, Tmp as load8

SignExtend8To32 U:G:8, ZD:G:32
    Tmp, Tmp as sign_extend8_to_32
    x86: Addr, Tmp as load8_signed_extend_to_32
    x86: Index, Tmp as load8_signed_extend_to_32

ZeroExtend16To32 U:G:16, ZD:G:32
    Tmp, Tmp as zero_extend16_to_32
    x86: Addr, Tmp as load16
    x86: Index, Tmp as load16

SignExtend16To32 U:G:16, ZD:G:32
    Tmp, Tmp as sign_extend16_to_32
    x86: Addr, Tmp as load16_signed_extend_to_32
    x86: Index, Tmp as load16_signed_extend_to_32

MoveFloat U:F:32, D:F:32
    Tmp, Tmp as move_double
    Addr, Tmp as load_float
    Index, Tmp as load_float
    Tmp, Addr as store_float
    Tmp, Index as store_float

MoveFloat U:F:32, D:F:32, S:F:32
    Addr, Addr, Tmp as move_float_rrr

MoveDouble U:F:64, D:F:64
    Tmp, Tmp as move_double 
    Addr, Tmp as load_double
    Index, Tmp as load_double
    Tmp, Addr as store_double
    Tmp, Index as store_double

MoveDouble U:F:64, D:F:64, S:F:64
    Addr, Addr, Tmp as move_double_rrr
    
MoveZeroToDouble D:F:64
    Tmp as move_zero_to_double

MoveZeroToFloat D:F:32
    Tmp as move_zero_to_float

64: Move64ToDouble U:G:64, D:F:64
    Tmp, Tmp as move64_to_double
    x86: Addr, Tmp as load_double
    Index, Tmp as load_double

32: Move64ToDouble U:G:32, U:G:32, D:F:64
    Tmp, Tmp, Tmp

32: Move32ToDoubleHi U:G:32, UD:F:64
    Tmp, Tmp

Move32ToFloat U:G:32, D:F:32
    Tmp, Tmp as move32_to_float
    x86: Addr, Tmp as load_float
    Index, Tmp as load_float

64: MoveDoubleTo64 U:F:64, D:G:64
    Tmp, Tmp as move_double_to64
    Addr, Tmp as load64
    Index, Tmp as load64

32: MoveDoubleTo64 U:F:64, D:G:32, D:G:32
    Tmp, Tmp, Tmp

32: MoveDoubleHiTo32 U:F:64, D:G:32
    Tmp, Tmp

MoveFloatTo32 U:F:32, D:G:32
    Tmp, Tmp as move_float_to32
    Addr, Tmp as load32
    Index, Tmp as load32

Load8 U:G:8, ZD:G:32
    Addr, Tmp as load8 
    Index, Tmp as load8

arm64: LoadAcq8 U:G:8, ZD:G:32 /effects
    SimpleAddr, Tmp

Store8 U:G:8, D:G:8
    Tmp, Index as store8 
    Tmp, Addr as store8 
    x86: Imm, Index as store8 
    x86: Imm, Addr as store8

arm64: StoreRel8 U:G:8, D:G:8 /effects
    Tmp, SimpleAddr

Load8SignedExtendTo32 U:G:8, ZD:G:32
    Addr, Tmp as load8_signed_extend_to_32
    Index, Tmp as load8_signed_extend_to_32

arm64: LoadAcq8SignedExtendTo32 U:G:8, ZD:G:32 /effects
    SimpleAddr, Tmp

Load16 U:G:16, ZD:G:32
    Addr, Tmp as load16 
    Index, Tmp as load16

arm64: LoadAcq16 U:G:16, ZD:G:32 /effects
    SimpleAddr, Tmp

Load16SignedExtendTo32 U:G:16, ZD:G:32
    Addr, Tmp as load16_signed_extend_to_32
    Index, Tmp as load16_signed_extend_to_32

arm64: LoadAcq16SignedExtendTo32 U:G:16, ZD:G:32 /effects
    SimpleAddr, Tmp

Store16 U:G:16, D:G:16
    Tmp, Index
    Tmp, Addr
    x86: Imm, Index
    x86: Imm, Addr

arm64: StoreRel16 U:G:16, D:G:16 /effects
    Tmp, SimpleAddr

arm64: LoadAcq32 U:G:32, ZD:G:32 /effects
    SimpleAddr, Tmp

arm64: StoreRel32 U:G:32, ZD:G:32 /effects
    Tmp, SimpleAddr

arm64: LoadAcq64 U:G:64, ZD:G:64 /effects
    SimpleAddr, Tmp

arm64: StoreRel64 U:G:64, ZD:G:64 /effects
    Tmp, SimpleAddr

x86: Xchg8 UD:G:8, UD:G:8 /effects
    Tmp, Addr as xchg8 
    Tmp, Index as xchg8 

x86: Xchg16 UD:G:16, UD:G:16 /effects
    Tmp, Addr as xchg16
    Tmp, Index as xchg16 

x86: Xchg32 UD:G:32, UD:G:32 /effects
    Tmp, Addr as xchg32 
    Tmp, Index as xchg32

x86_64: Xchg64 UD:G:64, UD:G:64 /effects
    Tmp, Addr as xchg64 
    Tmp, Index as xchg64

arm64: ExtractUnsignedBitfield32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Imm, Imm, Tmp

arm64: ExtractUnsignedBitfield64 U:G:64, U:G:32, U:G:32, D:G:64
    Tmp, Imm, Imm, Tmp

arm64: InsertUnsignedBitfieldInZero32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Imm, Imm, Tmp

arm64: InsertUnsignedBitfieldInZero64 U:G:64, U:G:32, U:G:32, D:G:64
    Tmp, Imm, Imm, Tmp

arm64: InsertBitField32 U:G:32, U:G:32, U:G:32, UZD:G:32
    Tmp, Imm, Imm, Tmp

arm64: InsertBitField64 U:G:64, U:G:32, U:G:32, UD:G:64
    Tmp, Imm, Imm, Tmp

arm64: ClearBitField32 U:G:32, U:G:32, ZD:G:32
    Imm, Imm, Tmp

arm64: ClearBitField64 U:G:32, U:G:32, D:G:64
    Imm, Imm, Tmp

arm64: ClearBitsWithMask32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp

arm64: ClearBitsWithMask64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp

arm64: ReverseBits64 U:G:64, D:G:64
    Tmp, Tmp

arm64: ReverseBits32 U:G:32, ZD:G:32
    Tmp, Tmp

arm64: OrNot32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp

arm64: OrNot64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp

arm64: XorNot32 U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Tmp

arm64: XorNot64 U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Tmp

arm64: XorNotLeftShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: XorNotRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: XorNotUnsignedRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: XorNotLeftShift64 U:G:64, U:G:64, U:G:32, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: XorNotRightShift64 U:G:64, U:G:64, U:G:32, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: XorNotUnsignedRightShift64 U:G:64, U:G:64, U:G:32, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: ExtractInsertBitfieldAtLowEnd32 U:G:32, U:G:32, U:G:32, UZD:G:32
    Tmp, Imm, Imm, Tmp

arm64: ExtractInsertBitfieldAtLowEnd64 U:G:64, U:G:32, U:G:32, UD:G:64
    Tmp, Imm, Imm, Tmp

arm64: InsertSignedBitfieldInZero32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Imm, Imm, Tmp

arm64: InsertSignedBitfieldInZero64 U:G:64, U:G:32, U:G:32, D:G:64
    Tmp, Imm, Imm, Tmp

arm64: ExtractSignedBitfield32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Imm, Imm, Tmp

arm64: ExtractSignedBitfield64 U:G:64, U:G:32, U:G:32, D:G:64
    Tmp, Imm, Imm, Tmp

arm64: ExtractRegister32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: ExtractRegister64 U:G:64, U:G:32, U:G:32, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: AddLeftShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: AddRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: AddUnsignedRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: AddLeftShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: AddRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: AddUnsignedRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: SubLeftShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: SubRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: SubUnsignedRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: SubLeftShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: SubRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: SubUnsignedRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: AndLeftShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: AndRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: AndUnsignedRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: AndLeftShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: AndRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: AndUnsignedRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: XorLeftShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: XorRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: XorUnsignedRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: XorLeftShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: XorRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: XorUnsignedRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: OrLeftShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: OrRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: OrUnsignedRightShift32 U:G:32, U:G:32, U:G:32, ZD:G:32
    Tmp, Tmp, Imm, Tmp

arm64: OrLeftShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: OrRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: OrUnsignedRightShift64 U:G:64, U:G:64, U:G:64, D:G:64
    Tmp, Tmp, Imm, Tmp

arm64: FloatMax U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp

arm64: FloatMin U:F:32, U:F:32, D:F:32
    Tmp, Tmp, Tmp

arm64: DoubleMax U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp

arm64: DoubleMin U:F:64, U:F:64, D:F:64
    Tmp, Tmp, Tmp

Compare32 U:G:32, U:G:32, U:G:32, ZD:G:32
    RelCond, Tmp, Tmp, Tmp
    RelCond, Tmp, Imm, Tmp

64: Compare64 U:G:32, U:G:64, U:G:64, ZD:G:32
    RelCond, Tmp, Tmp, Tmp
    x86: RelCond, Tmp, Imm, Tmp

Test32 U:G:32, U:G:32, U:G:32, ZD:G:32
    x86: ResCond, Addr, Imm, Tmp as test32_cond 
    ResCond, Tmp, Tmp, Tmp as test32_cond 
    ResCond, Tmp, BitImm, Tmp as test32_cond

64: Test64 U:G:32, U:G:64, U:G:64, ZD:G:32
    x86: ResCond, Tmp, Imm, Tmp
    ResCond, Tmp, Tmp, Tmp

CompareDouble U:G:32, U:F:64, U:F:64, ZD:G:32
    DoubleCond, Tmp, Tmp, Tmp

CompareFloat U:G:32, U:F:32, U:F:32, ZD:G:32
    DoubleCond, Tmp, Tmp, Tmp

# Note that branches have some logic in AirOptimizeBlockOrder.cpp. If you add new branches, please make sure
# you opt them into the block order optimizations.

Branch8 U:G:32, U:G:8, U:G:8 /branch
    x86: RelCond, Addr, Imm
    x86: RelCond, Index, Imm

Branch32 U:G:32, U:G:32, U:G:32 /branch
    x86: RelCond, Addr, Imm
    RelCond, Tmp, Tmp
    RelCond, Tmp, Imm
    x86: RelCond, Tmp, Addr
    x86: RelCond, Addr, Tmp
    x86: RelCond, Index, Imm

64: Branch64 U:G:32, U:G:64, U:G:64 /branch
    RelCond, Tmp, Tmp
    RelCond, Tmp, Imm
    x86: RelCond, Tmp, Addr
    x86: RelCond, Addr, Tmp
    x86: RelCond, Addr, Imm
    x86: RelCond, Index, Tmp

BranchTest8 U:G:32, U:G:8, U:G:8 /branch
    x86: ResCond, Addr, BitImm
    x86: ResCond, Index, BitImm

BranchTest32 U:G:32, U:G:32, U:G:32 /branch
    ResCond, Tmp, Tmp
    ResCond, Tmp, BitImm
    32: ResCond, Tmp, Imm
    x86: ResCond, Addr, BitImm
    x86: ResCond, Index, BitImm

# Warning: forms that take an immediate will sign-extend their immediate. You probably want
# BranchTest32 in most cases where you use an immediate.
64: BranchTest64 U:G:32, U:G:64, U:G:64 /branch
    ResCond, Tmp, Tmp
    arm64: ResCond, Tmp, BitImm64
    x86: ResCond, Tmp, BitImm
    x86: ResCond, Addr, BitImm
    x86: ResCond, Addr, Tmp
    x86: ResCond, Index, BitImm

x86_64: BranchTestBit64 U:G:32, U:G:64, U:G:8 /branch
    ResCond, Tmp, Imm
    ResCond, Addr, Imm
    ResCond, Tmp, Tmp

x86: BranchTestBit32 U:G:32, U:G:32, U:G:8 /branch
    ResCond, Tmp, Imm
    ResCond, Addr, Imm
    ResCond, Tmp, Tmp

BranchDouble U:G:32, U:F:64, U:F:64 /branch
    DoubleCond, Tmp, Tmp

BranchFloat U:G:32, U:F:32, U:F:32 /branch
    DoubleCond, Tmp, Tmp

BranchAdd32 U:G:32, U:G:32, U:G:32, ZD:G:32 /branch
    ResCond, Tmp, Tmp, Tmp as branch_add32_rrr
    x86:ResCond, Tmp, Addr, Tmp as branch_add32_rrr 
    x86:ResCond, Addr, Tmp, Tmp as branch_add32_rrr

BranchAdd32 U:G:32, U:G:32, UZD:G:32 /branch
    ResCond, Tmp, Tmp
    ResCond, Imm, Tmp
    x86: ResCond, Imm, Addr
    x86: ResCond, Tmp, Addr
    x86: ResCond, Addr, Tmp

64: BranchAdd64 U:G:32, U:G:64, U:G:64, ZD:G:64 /branch
    ResCond, Tmp, Tmp, Tmp as branch_add64_rrr
    x86:ResCond, Tmp, Addr, Tmp as branch_add64_rrr
    x86:ResCond, Addr, Tmp, Tmp as branch_add64_rrr

64: BranchAdd64 U:G:32, U:G:64, UD:G:64 /branch
    ResCond, Imm, Tmp
    ResCond, Tmp, Tmp
    x86:ResCond, Addr, Tmp

x86: BranchMul32 U:G:32, U:G:32, UZD:G:32 /branch
    ResCond, Tmp, Tmp
    ResCond, Addr, Tmp

x86: BranchMul32 U:G:32, U:G:32, U:G:32, ZD:G:32 /branch
    ResCond, Tmp, Imm, Tmp as branch_mul32_rrr

arm64: BranchMul32 U:G:32, U:G:32, U:G:32, S:G:32, S:G:32, ZD:G:32 /branch
    ResCond, Tmp, Tmp, Tmp, Tmp, Tmp

x86_64: BranchMul64 U:G:32, U:G:64, UZD:G:64 /branch
    ResCond, Tmp, Tmp

arm64: BranchMul64 U:G:32, U:G:64, U:G:64, S:G:64, S:G:64, ZD:G:64 /branch
    ResCond, Tmp, Tmp, Tmp, Tmp, Tmp

BranchSub32 U:G:32, U:G:32, UZD:G:32 /branch
    ResCond, Tmp, Tmp
    ResCond, Imm, Tmp
    x86: ResCond, Imm, Addr
    x86: ResCond, Tmp, Addr
    x86: ResCond, Addr, Tmp

64: BranchSub64 U:G:32, U:G:64, UD:G:64 /branch
    ResCond, Imm, Tmp
    ResCond, Tmp, Tmp

BranchNeg32 U:G:32, UZD:G:32 /branch
    ResCond, Tmp

64: BranchNeg64 U:G:32, UZD:G:64 /branch
    ResCond, Tmp

64: MoveConditionally32 U:G:32, U:G:32, U:G:32, U:G:Ptr, UD:G:Ptr
    RelCond, Tmp, Tmp, Tmp, Tmp

64: MoveConditionally32 U:G:32, U:G:32, U:G:32, U:G:Ptr, U:G:Ptr, D:G:Ptr
    RelCond, Tmp, Tmp, Tmp, Tmp, Tmp  as move_conditionally32_then_else
    RelCond, Tmp, Imm, Tmp, Tmp, Tmp  as move_conditionally32_then_else

64: MoveConditionally64 U:G:32, U:G:64, U:G:64, U:G:Ptr, UD:G:Ptr
    RelCond, Tmp, Tmp, Tmp, Tmp

64: MoveConditionally64 U:G:32, U:G:64, U:G:64, U:G:Ptr, U:G:Ptr, D:G:Ptr
    RelCond, Tmp, Tmp, Tmp, Tmp, Tmp as move_conditionally64_then_else
    RelCond, Tmp, Imm, Tmp, Tmp, Tmp as move_conditionally64_then_else

64: MoveConditionallyTest32 U:G:32, U:G:32, U:G:32, U:G:Ptr, UD:G:Ptr
    ResCond, Tmp, Tmp, Tmp, Tmp
    x86: ResCond, Tmp, Imm, Tmp, Tmp

64: MoveConditionallyTest32 U:G:32, U:G:32, U:G:32, U:G:Ptr, U:G:Ptr, D:G:Ptr
    ResCond, Tmp, Tmp, Tmp, Tmp, Tmp as move_conditionally_test32_then_else
    ResCond, Tmp, BitImm, Tmp, Tmp, Tmp as move_conditionally_test32_then_else

64: MoveConditionallyTest64 U:G:32, U:G:64, U:G:64, U:G:Ptr, UD:G:Ptr
    ResCond, Tmp, Tmp, Tmp, Tmp
    x86: ResCond, Tmp, Imm, Tmp, Tmp

64: MoveConditionallyTest64 U:G:32, U:G:32, U:G:32, U:G:Ptr, U:G:Ptr, D:G:Ptr
    ResCond, Tmp, Tmp, Tmp, Tmp, Tmp as move_conditionally_test64_then_else
    x86_64: ResCond, Tmp, Imm, Tmp, Tmp, Tmp as move_conditionally_test64_then_else

64: MoveConditionallyDouble U:G:32, U:F:64, U:F:64, U:G:Ptr, U:G:Ptr, D:G:Ptr
    DoubleCond, Tmp, Tmp, Tmp, Tmp, Tmp as move_conditionally_double_then_else

64: MoveConditionallyDouble U:G:32, U:F:64, U:F:64, U:G:Ptr, UD:G:Ptr
    DoubleCond, Tmp, Tmp, Tmp, Tmp

64: MoveConditionallyFloat U:G:32, U:F:32, U:F:32, U:G:Ptr, U:G:Ptr, D:G:Ptr
    DoubleCond, Tmp, Tmp, Tmp, Tmp, Tmp as move_conditionally_float_then_else

64: MoveConditionallyFloat U:G:32, U:F:32, U:F:32, U:G:Ptr, UD:G:Ptr
    DoubleCond, Tmp, Tmp, Tmp, Tmp

64: MoveDoubleConditionally32 U:G:32, U:G:32, U:G:32, U:F:64, U:F:64, D:F:64
    RelCond, Tmp, Tmp, Tmp, Tmp, Tmp
    RelCond, Tmp, Imm, Tmp, Tmp, Tmp
    x86: RelCond, Addr, Imm, Tmp, Tmp, Tmp
    x86: RelCond, Tmp, Addr, Tmp, Tmp, Tmp
    x86: RelCond, Addr, Tmp, Tmp, Tmp, Tmp
    x86: RelCond, Index, Imm, Tmp, Tmp, Tmp

64: MoveDoubleConditionally64 U:G:32, U:G:64, U:G:64, U:F:64, U:F:64, D:F:64
    RelCond, Tmp, Tmp, Tmp, Tmp, Tmp
    RelCond, Tmp, Imm, Tmp, Tmp, Tmp
    x86_64: RelCond, Tmp, Addr, Tmp, Tmp, Tmp
    x86_64: RelCond, Addr, Tmp, Tmp, Tmp, Tmp
    x86_64: RelCond, Addr, Imm, Tmp, Tmp, Tmp
    x86_64: RelCond, Index, Tmp, Tmp, Tmp, Tmp

64: MoveDoubleConditionallyTest32 U:G:32, U:G:32, U:G:32, U:F:64, U:F:64, D:F:64
    ResCond, Tmp, Tmp, Tmp, Tmp, Tmp
    ResCond, Tmp, BitImm, Tmp, Tmp, Tmp
    x86: ResCond, Addr, Imm, Tmp, Tmp, Tmp
    x86: ResCond, Index, Imm, Tmp, Tmp, Tmp

# Warning: forms that take an immediate will sign-extend their immediate. You probably want
# MoveDoubleConditionallyTest32 in most cases where you use an immediate.
64: MoveDoubleConditionallyTest64 U:G:32, U:G:64, U:G:64, U:F:64, U:F:64, D:F:64
    ResCond, Tmp, Tmp, Tmp, Tmp, Tmp
    x86_64: ResCond, Tmp, Imm, Tmp, Tmp, Tmp
    x86_64: ResCond, Addr, Imm, Tmp, Tmp, Tmp
    x86_64: ResCond, Addr, Tmp, Tmp, Tmp, Tmp
    x86_64: ResCond, Index, Imm, Tmp, Tmp, Tmp

64: MoveDoubleConditionallyDouble U:G:32, U:F:64, U:F:64, U:F:64, U:F:64, D:F:64
    DoubleCond, Tmp, Tmp, Tmp, Tmp, Tmp

64: MoveDoubleConditionallyFloat U:G:32, U:F:32, U:F:32, U:F:64, U:F:64, D:F:64
    DoubleCond, Tmp, Tmp, Tmp, Tmp, Tmp

MemoryFence /effects
StoreFence /effects
LoadFence /effects

Jump /branch

RetVoid /return

Ret32 U:G:32 /return
    Tmp

64: Ret64 U:G:64 /return
    Tmp

RetFloat U:F:32 /return
    Tmp

RetDouble U:F:64 /return
    Tmp

Oops /terminal

# This is a terminal but we express it as a Custom because we don't want it to have a code
# generator.
custom EntrySwitch

# A Shuffle is a multi-source, multi-destination move. It simultaneously does multiple moves at once.
# The moves are specified as triplets of src, dst, and width. For example you can request a swap this
# way:
#     Shuffle %tmp1, %tmp2, 64, %tmp2, %tmp1, 64
custom Shuffle

# Air allows for exotic behavior. A Patch's behavior is determined entirely by the Special operand,
# which must be the first operand.
custom Patch

# Instructions used for lowering C calls. These don't make it to Air generation. They get lowered to
# something else first. The origin Value must be a CCallValue.
custom CCall
custom ColdCCall

