64: MoveVector U:F:128, D:F:128
    Tmp, Tmp
    Addr, Tmp as loadVector
    Index, Tmp as loadVector
    Tmp, Addr as storeVector
    Tmp, Index as storeVector


# SIMD
64: VectorReplaceLaneInt64 U:G:8, U:G:64, UD:F:128
    Imm, Tmp, Tmp
64: VectorReplaceLaneInt32 U:G:8, U:G:32, UD:F:128
    Imm, Tmp, Tmp
64: VectorReplaceLaneInt16 U:G:8, U:G:16, UD:F:128
    Imm, Tmp, Tmp
64: VectorReplaceLaneInt8 U:G:8, U:G:8, UD:F:128
    Imm, Tmp, Tmp
64: VectorReplaceLaneFloat64 U:G:8, U:F:64, UD:F:128
    Imm, Tmp, Tmp
64: VectorReplaceLaneFloat32 U:G:8, U:F:32, UD:F:128
    Imm, Tmp, Tmp

64: VectorExtractLaneInt64 U:G:8, U:F:128, D:G:64
    Imm, Tmp, Tmp
64: VectorExtractLaneInt32 U:G:8, U:F:128, ZD:G:32
    Imm, Tmp, Tmp
64: VectorExtractLaneSignedInt16 U:G:8, U:F:128, ZD:G:32
    Imm, Tmp, Tmp
64: VectorExtractLaneUnsignedInt16 U:G:8, U:F:128, ZD:G:16
    Imm, Tmp, Tmp
64: VectorExtractLaneSignedInt8 U:G:8, U:F:128, ZD:G:32
    Imm, Tmp, Tmp
64: VectorExtractLaneUnsignedInt8 U:G:8, U:F:128, ZD:G:8
    Imm, Tmp, Tmp
64: VectorExtractLaneFloat64 U:G:8, U:F:128, D:F:64
    Imm, Tmp, Tmp
64: VectorExtractLaneFloat32 U:G:8, U:F:128, D:F:32
    Imm, Tmp, Tmp

64: VectorSplatInt8 U:G:8, D:F:128
    Tmp, Tmp
64: VectorSplatInt16 U:G:16, D:F:128
    Tmp, Tmp
64: VectorSplatInt32 U:G:32, D:F:128
    Tmp, Tmp
64: VectorSplatInt64 U:G:64, D:F:128
    Tmp, Tmp
64: VectorSplatFloat32 U:F:32, D:F:128
    Tmp, Tmp
64: VectorSplatFloat64 U:F:64, D:F:128
    Tmp, Tmp

x86_64: CompareFloatingPointVectorUnordered U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: CompareFloatingPointVector U:G:32, U:G:Ptr, U:F:128, U:F:128, D:F:128
    DoubleCond, SIMDInfo, Tmp, Tmp, Tmp

arm64: CompareIntegerVector U:G:32, U:G:Ptr, U:F:128, U:F:128, D:F:128
    RelCond, SIMDInfo, Tmp, Tmp, Tmp

x86_64: CompareIntegerVector U:G:32, U:G:Ptr, U:F:128, U:F:128, D:F:128, S:F:128
    RelCond, SIMDInfo, Tmp, Tmp, Tmp, Tmp

arm64: CompareIntegerVectorWithZero U:G:32, U:G:Ptr, U:F:128, D:F:128
    RelCond, SIMDInfo, Tmp, Tmp

x86_64: CompareIntegerVectorWithZero U:G:32, U:G:Ptr, U:F:128, D:F:128, S:G:8
    RelCond, SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorUnsignedMax U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

arm64: VectorUnsignedMin U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorAdd U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorSub U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorAddSat U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorSubSat U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorMul U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorMulByElementFloat32 U:F:128, U:F:128, U:G:8, D:F:128
    Tmp, Tmp, Imm, Tmp

arm64: VectorMulByElementFloat64 U:F:128, U:F:128, U:G:8, D:F:128
    Tmp, Tmp, Imm, Tmp

64: VectorDiv U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorMin U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorMax U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorPmin U:G:Ptr, U:F:128, U:F:128, D:F:128, S:F:128
    SIMDInfo, Tmp, Tmp, Tmp, Tmp

arm64: VectorPmax U:G:Ptr, U:F:128, U:F:128, D:F:128, S:F:128
    SIMDInfo, Tmp, Tmp, Tmp, Tmp

x86_64: VectorPmin U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

x86_64: VectorPmax U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorNarrow U:G:Ptr, U:F:128, U:F:128, D:F:128, S:F:128
    SIMDInfo, Tmp, Tmp, Tmp, Tmp

64: VectorBitwiseSelect U:F:128, U:F:128, UD:F:128
    Tmp, Tmp, Tmp

arm64: VectorNot U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorAnd U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorAndnot U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorOr U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorXor U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: MoveZeroToVector D:F:128
    Tmp

64: VectorUshl U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

x86_64: VectorSshr U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

x86_64: VectorUshr U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorSshl U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

x86_64: VectorUshl8 U:F:128, U:F:128, D:F:128, S:F:128, S:F:128
    Tmp, Tmp, Tmp, Tmp, Tmp

x86_64: VectorUshr8 U:F:128, U:F:128, D:F:128, S:F:128, S:F:128
    Tmp, Tmp, Tmp, Tmp, Tmp

x86_64: VectorSshr8 U:F:128, U:F:128, D:F:128, S:F:128, S:F:128
    Tmp, Tmp, Tmp, Tmp, Tmp

x86_64: VectorUshr8 U:G:Ptr, U:F:128, U:G:8, D:F:128
    SIMDInfo, Tmp, Imm, Tmp

64: VectorSshr8 U:G:Ptr, U:F:128, U:G:8, D:F:128
    SIMDInfo, Tmp, Imm, Tmp

arm64: VectorHorizontalAdd U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

arm64: VectorZipUpper U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorUnzipEven U:G:Ptr, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorExtractPair U:G:Ptr, U:G:8, U:F:128, U:F:128, D:F:128
    SIMDInfo, Imm, Tmp, Tmp, Tmp

64: VectorAbs U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

x86_64: VectorAbsInt64 U:F:128, D:F:128, S:F:128
    Tmp, Tmp, Tmp

arm64: VectorNeg U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

arm64: VectorPopcnt U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorCeil U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorFloor U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorTrunc U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

arm64: VectorTruncSat U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

x86_64: VectorTruncSat U:G:Ptr, U:F:128, D:F:128, S:G:64, S:F:128, S:F:128
    SIMDInfo, Tmp, Tmp, Tmp, Tmp, Tmp

x86_64: VectorTruncSatUnsignedFloat32 U:F:128, D:F:128, S:G:64, S:F:128, S:F:128
    Tmp, Tmp, Tmp, Tmp, Tmp

x86_64: VectorTruncSatSignedFloat64 U:F:128, D:F:128, S:G:64, S:F:128
    Tmp, Tmp, Tmp, Tmp

x86_64: VectorTruncSatUnsignedFloat64 U:F:128, D:F:128, S:G:64, S:F:128
    Tmp, Tmp, Tmp, Tmp

64: VectorConvert U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

x86_64: VectorConvertUnsigned U:F:128, D:F:128, S:F:128
    Tmp, Tmp, Tmp

arm64: VectorConvertLow U:G:Ptr, U:F:64, D:F:128
    SIMDInfo, Tmp, Tmp

x86_64: VectorConvertLowSignedInt32 U:F:64, D:F:128
    Tmp, Tmp

x86_64: VectorConvertLowUnsignedInt32 U:F:64, D:F:128, S:G:64, S:F:128
    Tmp, Tmp, Tmp, Tmp

64: VectorNearest U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorSqrt U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorExtendLow U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorExtendHigh U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorPromote U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

64: VectorDemote U:G:Ptr, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

arm64: VectorLoad8Splat U:G:8, D:F:128
    SimpleAddr, Tmp
x86_64: VectorLoad8Splat U:G:8, D:F:128, S:F:128
    SimpleAddr, Tmp, Tmp

64: VectorLoad16Splat U:G:16, D:F:128
    SimpleAddr, Tmp
64: VectorLoad32Splat U:G:32, D:F:128
    SimpleAddr, Tmp
64: VectorLoad64Splat U:G:64, D:F:128
    SimpleAddr, Tmp

64: VectorLoad8Lane U:G:8, U:G:8, UD:F:128
    SimpleAddr, Imm, Tmp
64: VectorLoad16Lane U:G:16, U:G:8, UD:F:128
    SimpleAddr, Imm, Tmp
64: VectorLoad32Lane U:G:32, U:G:8, UD:F:128
    SimpleAddr, Imm, Tmp
64: VectorLoad64Lane U:G:64, U:G:8, UD:F:128
    SimpleAddr, Imm, Tmp

64: VectorStore8Lane U:F:128, U:G:8, U:G:8
    Tmp, SimpleAddr, Imm
64: VectorStore16Lane U:F:128, U:G:16, U:G:8
    Tmp, SimpleAddr, Imm
64: VectorStore32Lane U:F:128, U:G:32, U:G:8
    Tmp, SimpleAddr, Imm
64: VectorStore64Lane U:F:128, U:G:64, U:G:8
    Tmp, SimpleAddr, Imm

64: VectorAnyTrue U:F:128, ZD:G:32
    Tmp, Tmp

x86_64: VectorAllTrue U:G:8, U:F:128, ZD:G:32, S:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorAllTrue U:G:8, U:F:128, ZD:G:32
    SIMDInfo, Tmp, Tmp

arm64: VectorBitmask U:G:8, U:F:128, ZD:G:32
    SIMDInfo, Tmp, Tmp

x86_64: VectorBitmask U:G:8, U:F:128, ZD:G:32, S:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorExtaddPairwise U:G:8, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp

x86_64: VectorExtaddPairwise U:G:8, U:F:128, D:F:128, S:G:64, S:F:128
    SIMDInfo, Tmp, Tmp, Tmp, Tmp

x86_64: VectorExtaddPairwiseUnsignedInt16 U:F:128, D:F:128, S:F:128
    Tmp, Tmp, Tmp

arm64: VectorAddPairwise U:G:8, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

64: VectorAvgRound U:G:8, U:F:128, U:F:128, D:F:128
    SIMDInfo, Tmp, Tmp, Tmp

arm64: VectorMulSat U:F:128, U:F:128, D:F:128
    Tmp, Tmp, Tmp

x86_64: VectorMulSat U:F:128, U:F:128, D:F:128, S:G:64, S:F:128
    Tmp, Tmp, Tmp, Tmp, Tmp

arm64: VectorDotProduct U:F:128, U:F:128, D:F:128, S:F:128
    Tmp, Tmp, Tmp, Tmp

x86_64: VectorDotProduct U:F:128, U:F:128, D:F:128
    Tmp, Tmp, Tmp

64: VectorSwizzle U:F:128, U:F:128, D:F:128
    Tmp, Tmp, Tmp

arm64: VectorSwizzle2 U:F:128, U:F:128, U:F:128, D:F:128
    Tmp, Tmp*, Tmp, Tmp

arm64: VectorDupElementInt8 U:G:8, U:F:128, D:F:128
    Imm, Tmp, Tmp
arm64: VectorDupElementInt16 U:G:8, U:F:128, D:F:128
    Imm, Tmp, Tmp
arm64: VectorDupElementInt32 U:G:8, U:F:128, D:F:128
    Imm, Tmp, Tmp
arm64: VectorDupElementInt64 U:G:8, U:F:128, D:F:128
    Imm, Tmp, Tmp
arm64: VectorDupElementFloat32 U:G:8, U:F:128, D:F:128
    Imm, Tmp, Tmp
arm64: VectorDupElementFloat64 U:G:8, U:F:128, D:F:128
    Imm, Tmp, Tmp


# The first operand is rax.
# FIXME: This formulation means that the boolean result cannot be put in eax, even though all users
# of this would be OK with that.
# https://bugs.webkit.org/show_bug.cgi?id=169254
x86: AtomicStrongCAS8 U:G:32, UD:G:8, U:G:8, UD:G:8, ZD:G:8 /effects
    StatusCond, Tmp*, Tmp, Addr, Tmp
    StatusCond, Tmp*, Tmp, Index, Tmp

x86: AtomicStrongCAS16 U:G:32, UD:G:16, U:G:32, UD:G:16, ZD:G:8 /effects
    StatusCond, Tmp*, Tmp, Addr, Tmp
    StatusCond, Tmp*, Tmp, Index, Tmp

x86: AtomicStrongCAS32 U:G:32, UD:G:32, U:G:32, UD:G:32, ZD:G:8 /effects
    StatusCond, Tmp*, Tmp, Addr, Tmp
    StatusCond, Tmp*, Tmp, Index, Tmp

x86_64: AtomicStrongCAS64 U:G:32, UD:G:64, U:G:64, UD:G:64, ZD:G:8 /effects
    StatusCond, Tmp*, Tmp, Addr, Tmp
    StatusCond, Tmp*, Tmp, Index, Tmp

x86 arm64_lse: AtomicStrongCAS8 UD:G:8, U:G:8, UD:G:8 /effects
    x86: Tmp*, Tmp, Addr
    x86: Tmp*, Tmp, Index
    arm64_lse: Tmp, Tmp, SimpleAddr

x86 arm64_lse: AtomicStrongCAS16 UD:G:16, U:G:32, UD:G:16 /effects
    x86: Tmp*, Tmp, Addr
    x86: Tmp*, Tmp, Index
    arm64_lse: Tmp, Tmp, SimpleAddr

x86 arm64_lse: AtomicStrongCAS32 UD:G:32, U:G:32, UD:G:32 /effects
    x86: Tmp*, Tmp, Addr
    x86: Tmp*, Tmp, Index
    arm64_lse: Tmp, Tmp, SimpleAddr

x86_64 arm64_lse: AtomicStrongCAS64 UD:G:64, U:G:64, UD:G:64 /effects
    x86_64: Tmp*, Tmp, Addr
    x86_64: Tmp*, Tmp, Index
    arm64_lse: Tmp, Tmp, SimpleAddr

x86: BranchAtomicStrongCAS8 U:G:32, UD:G:8, U:G:8, UD:G:8 /branch /effects
    StatusCond, Tmp*, Tmp, Addr
    StatusCond, Tmp*, Tmp, Index

x86: BranchAtomicStrongCAS16 U:G:32, UD:G:16, U:G:32, UD:G:16 /branch /effects
    StatusCond, Tmp*, Tmp, Addr
    StatusCond, Tmp*, Tmp, Index

x86: BranchAtomicStrongCAS32 U:G:32, UD:G:32, U:G:32, UD:G:32 /branch /effects
    StatusCond, Tmp*, Tmp, Addr
    StatusCond, Tmp*, Tmp, Index

x86_64: BranchAtomicStrongCAS64 U:G:32, UD:G:64, U:G:64, UD:G:64 /branch /effects
    StatusCond, Tmp*, Tmp, Addr
    StatusCond, Tmp*, Tmp, Index

x86: AtomicAdd8 U:G:8, UD:G:8 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicAdd16 U:G:16, UD:G:16 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicAdd32 U:G:32, UD:G:32 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86_64: AtomicAdd64 U:G:64, UD:G:64 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicSub8 U:G:8, UD:G:8 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicSub16 U:G:16, UD:G:16 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicSub32 U:G:32, UD:G:32 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86_64: AtomicSub64 U:G:64, UD:G:64 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicAnd8 U:G:8, UD:G:8 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicAnd16 U:G:16, UD:G:16 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicAnd32 U:G:32, UD:G:32 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86_64: AtomicAnd64 U:G:64, UD:G:64 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicOr8 U:G:8, UD:G:8 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicOr16 U:G:16, UD:G:16 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicOr32 U:G:32, UD:G:32 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86_64: AtomicOr64 U:G:64, UD:G:64 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicXor8 U:G:8, UD:G:8 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicXor16 U:G:16, UD:G:16 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicXor32 U:G:32, UD:G:32 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86_64: AtomicXor64 U:G:64, UD:G:64 /effects
    Imm, Addr
    Imm, Index
    Tmp, Addr
    Tmp, Index

x86: AtomicNeg8 UD:G:8 /effects
    Addr
    Index

x86: AtomicNeg16 UD:G:16 /effects
    Addr
    Index

x86: AtomicNeg32 UD:G:32 /effects
    Addr
    Index

x86_64: AtomicNeg64 UD:G:64 /effects
    Addr
    Index

x86: AtomicNot8 UD:G:8 /effects
    Addr
    Index

x86: AtomicNot16 UD:G:16 /effects
    Addr
    Index

x86: AtomicNot32 UD:G:32 /effects
    Addr
    Index

x86_64: AtomicNot64 UD:G:64 /effects
    Addr
    Index

x86: AtomicXchgAdd8 UD:G:8, UD:G:8 /effects
    Tmp, Addr
    Tmp, Index

x86: AtomicXchgAdd16 UD:G:16, UD:G:16 /effects
    Tmp, Addr
    Tmp, Index

x86: AtomicXchgAdd32 UD:G:32, UD:G:32 /effects
    Tmp, Addr
    Tmp, Index

x86_64: AtomicXchgAdd64 UD:G:64, UD:G:64 /effects
    Tmp, Addr
    Tmp, Index

x86: AtomicXchg8 UD:G:8, UD:G:8 /effects
    Tmp, Addr
    Tmp, Index

x86: AtomicXchg16 UD:G:16, UD:G:16 /effects
    Tmp, Addr
    Tmp, Index

x86: AtomicXchg32 UD:G:32, UD:G:32 /effects
    Tmp, Addr
    Tmp, Index

x86_64: AtomicXchg64 UD:G:64, UD:G:64 /effects
    Tmp, Addr
    Tmp, Index

arm: LoadLink8 U:G:8, ZD:G:8 /effects
    SimpleAddr, Tmp

arm64: LoadLinkAcq8 U:G:8, ZD:G:8 /effects
    SimpleAddr, Tmp

# Super confusing fact: this returns 0 to mean success, 1 to mean failure.
arm: StoreCond8 U:G:8, D:G:8, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

arm64: StoreCondRel8 U:G:8, D:G:8, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

arm: LoadLink16 U:G:16, ZD:G:16 /effects
    SimpleAddr, Tmp

arm64: LoadLinkAcq16 U:G:16, ZD:G:16 /effects
    SimpleAddr, Tmp

arm: StoreCond16 U:G:16, D:G:16, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

arm64: StoreCondRel16 U:G:16, D:G:16, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

arm: LoadLink32 U:G:32, ZD:G:32 /effects
    SimpleAddr, Tmp

arm64: LoadLinkAcq32 U:G:32, ZD:G:32 /effects
    SimpleAddr, Tmp

arm: StoreCond32 U:G:32, D:G:32, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

arm64: StoreCondRel32 U:G:32, D:G:32, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

armv7: LoadLinkPair32 U:G:64, D:G:32, D:G:32 /effects
    SimpleAddr, Tmp, Tmp

arm64: LoadLink64 U:G:64, ZD:G:64 /effects
    SimpleAddr, Tmp

arm64: LoadLinkAcq64 U:G:64, ZD:G:64 /effects
    SimpleAddr, Tmp

armv7: StoreCondPair32 U:G:32, U:G:32, D:G:64, EZD:G:8 /effects
    Tmp, Tmp, SimpleAddr, Tmp

arm64: StoreCond64 U:G:64, D:G:64, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

arm64: StoreCondRel64 U:G:64, D:G:64, EZD:G:8 /effects
    Tmp, SimpleAddr, Tmp

arm64: Depend32 U:G:32, ZD:G:32
    Tmp, Tmp

arm64: Depend64 U:G:64, ZD:G:64
    Tmp, Tmp

arm64_lse: AtomicXchgAdd8 U:G:8, UD:G:8, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgAdd16 U:G:16, UD:G:16, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgAdd32 U:G:32, UD:G:32, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgAdd64 U:G:64, UD:G:64, ZD:G:64 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgOr8 U:G:8, UD:G:8, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgOr16 U:G:16, UD:G:16, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgOr32 U:G:32, UD:G:32, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgOr64 U:G:64, UD:G:64, ZD:G:64 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgClear8 U:G:8, UD:G:8, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgClear16 U:G:16, UD:G:16, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgClear32 U:G:32, UD:G:32, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgClear64 U:G:64, UD:G:64, ZD:G:64 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgXor8 U:G:8, UD:G:8, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgXor16 U:G:16, UD:G:16, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgXor32 U:G:32, UD:G:32, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchgXor64 U:G:64, UD:G:64, ZD:G:64 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchg8 U:G:8, UD:G:8, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchg16 U:G:16, UD:G:16, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchg32 U:G:32, UD:G:32, ZD:G:32 /effects
    Tmp, SimpleAddr, Tmp

arm64_lse: AtomicXchg64 U:G:64, UD:G:64, ZD:G:64 /effects
    Tmp, SimpleAddr, Tmp